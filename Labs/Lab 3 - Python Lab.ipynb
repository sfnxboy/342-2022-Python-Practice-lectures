{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Python Lab\n",
    "## Author: *your name here*\n",
    "## Date: *Check Github*\n",
    "\n",
    "#### Support Vector Machine vs. Perceptron\n",
    "\n",
    "1. Import sklearn, pandas and numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recreate the data from the previous lab and visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df\n",
    "df = pd.DataFrame({'response': [0, 0, 0, 1, 1, 1],\n",
    "                   'first_feature': [1, 1, 2, 3, 3, 4],\n",
    "                   'second_feature': [1, 2, 1, 3, 4, 3]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll do the same with matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Output plot\n",
    "scatter =plt.scatter(x =df['first_feature'],\n",
    "                     y=df['second_feature'], \n",
    "                     c = df['response'],\n",
    "                    )\n",
    "\n",
    "# Setting axis labels\n",
    "plt.xlabel('first_feature')\n",
    "plt.ylabel('second_feature')\n",
    "\n",
    "#add legend\n",
    "plt.legend(*scatter.legend_elements())\n",
    "\n",
    "# Display plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Is the datra linearly seperable?\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "3. Define X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create an SVM classifier object with the `svm` library from sklearn, and fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Output the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plot Decision Region using mlxtend's awesome plotting function. If you have trouble with this, refer to Python Practice Demo 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Now write pseuocode for your own implementation of the linear support vector machine algorithm using the Vapnik objective function we discussed.\n",
    "\n",
    "Note there are differences between this spec and the perceptron learning algorithm spec in the previous lab. You should figure out a way to respect the `MAX_ITER` argument value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm_learning_algorithm(X_input, y_binary, MAX_ITER = 5000, lambda_val = 1):\n",
    "    \"\"\"\n",
    "    Support Vector Machine \n",
    "    This function implements the hinge-loss + maximum margin linear support\n",
    "    vector machine algorithm of Vladimir Vapnik (1963).\n",
    "    \n",
    "    X_input: The training data features as an n x p matrix.\n",
    "    y_binary: The training data responses as a vector of length n consisting of only 0's and 1's.\n",
    "    MAX_ITER: The maximum number of iterations the algorithm performs. Defaults to 5000.\n",
    "    lambda_val: A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss. The default value is 1.\n",
    "    \n",
    "    return: *describe*\n",
    "    \"\"\"\n",
    "    # TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. For the previous question you wrote psuedocode for the procedure, now code the function. If you are enrolled in 342W the following is extra credit but if you're enrolled in 650, the following is required. Write the actual code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_svm_learning_algorithm(X_input, y_binary, MAX_ITER = 5000, lambda_val = 1):\n",
    "    \"\"\"\n",
    "    Support Vector Machine \n",
    "    This function implements the hinge-loss + maximum margin linear support\n",
    "    vector machine algorithm of Vladimir Vapnik (1963).\n",
    "    \n",
    "    X_input: The training data features as an n x p matrix.\n",
    "    y_binary: The training data responses as a vector of length n consisting of only 0's and 1's.\n",
    "    MAX_ITER: The maximum number of iterations the algorithm performs. Defaults to 5000.\n",
    "    lambda_val: A scalar hyperparameter trading off margin of the hyperplane versus average hinge loss. The default value is 1.\n",
    "    \n",
    "    return: *describe*\n",
    "    \"\"\"\n",
    "    # TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Show that your function works as expected by comparing it to sci-kit learn's svm library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinary Least Squares\n",
    "\n",
    "We now move on to simple linear modeling using the ordinary least squares algorithm.\n",
    "\n",
    "Let's quickly recreate the sample data set from practice lecture 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "x = np.random.uniform(0, 1, n)\n",
    "\n",
    "# We'll set beta_0 and beta_1\n",
    "beta_0 = 3\n",
    "beta_1 = -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Compute h^* as `h_star_x`, then draw epsilon from an iid N(0, 0.33^2) distribution as `epsilon`, then compute the vector y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Plot the line `h_star_x` on a scatterplot with x and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. Write a function `my_simple_ols` that takes in a vector `x` and vector `y` and returns a list that contains the `b_0` (intercept), `b_1` (slope), `yhat` (the predictions), `e` (the residuals), `SSE`, `SST`, `MSE`, `RMSE` and `Rsq` (for the R-squared metric). Internally, you can only use the functions `sum` and `length` and other basic arithmetic operations. You should throw errors if the inputs are non-numeric or not the same length. No need to create documentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Output b_0 and b_1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Run another OLS model using the same `x` and `y`, this time using the sklearn `LinearRegression` module. Compare the coefficients, verify they are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Verify that the average of the residuals is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Create a prediction method `g` that takes in a vector `x_star` and `my_simple_ols`, an object of type `my_simple_ols` and predicts y values for each entry in `x_star`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. In class we spoke about error due to ignorance, misspecification error and estimation error. Show that as n grows, estimation error shrinks. Let us define an error metric that is the difference between b_0 and b_1 and beta_0 and beta_1. How about ||b - beta||^2 where the quantities are now the vectors of size two. Show as n increases, this shrinks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the Galton data, and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for Galton Height Data\n",
    "url = \"https://raw.github.com/vincentarelbundock/Rdatasets/master/csv/HistData/Galton.csv\"\n",
    "galton_df = pd.read_csv(url)\n",
    "\n",
    "galton_df = galton_df.iloc[:, 1:3]\n",
    "\n",
    "galton_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing seaborn\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# add jitter to object\n",
    "def jitter(values, j = 0):\n",
    "    return values + np.random.normal(j, 0.1, values.shape)\n",
    "\n",
    "sns.scatterplot(x = jitter(galton_df[\"parent\"]), y = jitter(galton_df[\"child\"]),\n",
    "                alpha = 0.5,  color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Output summary statistics for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. Find the average height (include both parents and children in this computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20. If you were predicting child height from parent and you were using the null model, what would the RMSE be of this model be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. If you were predicting child height from parent and you were using the null model, what would the RMSE be of this model be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in Math 241 you learned that the sample average is an estimate of the \"mean\", the population expected value of height. We will call the average the \"mean\" going forward since it is probably correct to the nearest tenth of an inch with this amount of data.\n",
    "\n",
    "22. Run a linear model attempting to explain the childrens' height using the parents' height. Use `LinearRegression`. Compute and report $b_0$, $b_1$, RMSE and $R^2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Interpret all four quantities: $b_0$, $b_1$, RMSE and $R^2$. Use the correct units of these metrics in your answer.\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "24. How good is this model? How well does it predict? Discuss.\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "25. It is reasonable to assume that parents and their children have the same height? Explain why this is reasonable using basic biology and common sense.\n",
    "\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "26. If they were to have the same height and any differences were just random noise with expectation 0, what would the values of beta_0 and beta_1 be?\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "27. Let's plot (a) the data in D as black dots, (b) your least squares line defined by b_0 and b_1 in blue, (c) the theoretical line beta_0 and beta_1 if the parent-child height equality held in red and (d) the mean height in green."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. Fill in the following sentence: \n",
    "\n",
    "Children of short parents became *...* on average and children of tall parents became *...* on average.\n",
    "\n",
    "29. Why did Galton call it \"Regression towards mediocrity in hereditary stature\" which was later shortened to \"regression to the mean\"?\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "30. Why should this effect be real?\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "31. You now have unlocked the mystery. Why is it that when modeling with y continuous, everyone calls it \"regression\"? Write a better, more descriptive and appropriate name for building predictive models with y continuous.\n",
    "\n",
    "*TO-DO*\n",
    "\n",
    "32. Create a dataset $\\mathbb{D}$ which we call `Xy` such that the linear model as $R^2$ about 50\\% and RMSE approximately 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33. Create a dataset $\\mathbb{D}$ which we call `Xy` such that the linear model as $R^2$ about 0\\% but `x`, `y` are clearly associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "34. Extra credit: create a dataset $\\mathbb{D}$ and a model that can give you $R^2$ arbitrarily close to 1 i.e. approximately 1 - epsilon but RMSE arbitrarily high i.e. approximately M."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "35. Write a function `my_ols` that takes in `X`, a matrix with with p columns representing the feature measurements for each of the n units, a vector of n responses `y` and returns a list that contains the `b`, the p+1-sized column vector of OLS coefficients, `yhat` (the vector of n predictions), `e` (the vector of n residuals), `df` for degrees of freedom of the model, `SSE`, `SST`, `MSE`, `RMSE` and `Rsq` (for the R-squared metric). Internally, you cannot use `LinearRegression` or any other package; it must be done manually. You should throw errors if the inputs are non-numeric or not the same length. Or if `X` is not otherwise suitable. No need to create documentation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "36. Verify that the OLS coefficients for the `Type` of cars in the cars dataset gives you the same results as we did in class (i.e. the $\\bar{y}$'s within group). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "37. Create a prediction method `g` that takes in a vector `x_star` and the dataset D i.e. `X` and `y` and returns the OLS predictions. Let `X` be a matrix with with p columns representing the feature measurements for each of the n units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
